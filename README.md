# ğŸ¤– RAG_MCP_CODE_SEARCH

**RAG_MCP_CODE_SEARCH** is an intelligent **AI-driven code retrieval and modification system** that leverages **RAG (Retrieval-Augmented Generation)** with **MCP (Model Context Protocol)**. It enables seamless **code search, draft generation, and automatic synchronization** between your **local workspace** and **Vector Database**.

---

## ğŸ§  Overview

This system provides an end-to-end solution for intelligent code management using the Model Context Protocol:

- ğŸ“¦ **Upload and index** zipped code projects into a Vector Database (one-time setup)
- ğŸ” **Query relevant code snippets** using natural language powered by RAG
- âœï¸ **Generate code modifications** automatically via MCP tools
- âœ… **Review and approve** changes before applying them to your local folder and Vector DB
- ğŸ”Œ **MCP Server**: Streamable HTTP server exposing two custom MCP tools
- ğŸ¤– **Anthropic Client**: Communicates with MCP tools for intelligent code operations
- âš™ï¸ **FastAPI + Streamlit architecture** for robust backend-frontend integration

ğŸ¥ **Demo Video:** [Watch on Loom](https://www.loom.com/share/492fe76b6c9c410f921f5fdae06d4bfa)

---

## âœ¨ Key Features

- ğŸ”— **MCP Integration**: Implements Model Context Protocol for tool-based AI interactions
- ğŸ§  **Semantic Search**: Powered by Vector Database (Zilliz) for accurate code retrieval
- ğŸ’¬ **Natural Language Interface**: Query your codebase using plain English
- ğŸ§¾ **Draft-Before-Apply**: Review generated modifications before committing changes
- ğŸ”„ **Automatic Synchronization**: Keep your local folder and Vector DB in perfect sync
- ğŸ› ï¸ **Two Custom MCP Tools**:
  - **RAG Search Tool**: Semantic code search and retrieval
  - **Code Update Tool**: Updates code in local folder and vector embeddings upon approval
- ğŸŒ **MCP Streamable HTTP Server**: Exposes tools via HTTP for flexible integration
- ğŸ¤– **Anthropic Client**: Built-in client for seamless communication with MCP tools
- âš™ï¸ **Modular Architecture**: Extensible and production-ready design

---

## ğŸ—ï¸ Project Structure

```
RAG_MCP_CODE_SEARCH/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ uploads/              # Folder for uploaded code zips & extracted files
â”‚   â”œâ”€â”€ files.py              # File handling utilities
â”‚   â””â”€â”€ uploads.py            # Upload processing logic
â”‚
â”œâ”€â”€ mcp/
â”‚   â”œâ”€â”€ server.py             # MCP Streamable HTTP Server
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ rag_search.py     # RAG Search MCP Tool
â”‚   â”‚   â””â”€â”€ code_update.py    # Code Update MCP Tool
â”‚   â””â”€â”€ client.py             # Anthropic Client for MCP communication
â”‚
â”œâ”€â”€ server.py                 # FastAPI backend (API endpoints, VectorDB handling)
â”œâ”€â”€ streamlit.py              # Streamlit frontend for user interaction
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ .env                      # Environment variables (see configuration below)
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ”§ MCP Architecture

### Model Context Protocol Implementation

This project implements the **Model Context Protocol (MCP)**, which enables structured communication between AI models and external tools.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Anthropic      â”‚
â”‚  Claude Client  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ MCP Protocol
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP HTTP Server    â”‚
â”‚  (Streamable)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG   â”‚ â”‚   Code     â”‚
â”‚ Search â”‚ â”‚   Update   â”‚
â”‚  Tool  â”‚ â”‚    Tool    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚
     â†“           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Database   â”‚
â”‚   Local File System â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MCP Tools

#### 1. **RAG Search Tool** (`rag_search`)
- **Purpose**: Semantic search through your codebase
- **Input**: Natural language query
- **Output**: Relevant code snippets with context
- **Process**: 
  - Converts query to embeddings
  - Searches Vector DB for similar code
  - Returns ranked results with file paths

#### 2. **Code Update Tool** (`code_update`)
- **Purpose**: Updates code after user approval
- **Input**: File path, new code content, approval status
- **Output**: Confirmation of updates
- **Process**:
  - Updates local file system
  - Regenerates embeddings
  - Updates Vector DB
  - Maintains sync between both stores

---

## ğŸ” Environment Configuration

Create a `.env` file in the root directory with the following configuration:

```bash
# OpenAI API Configuration
OPENAI_API_KEY="your-openai-api-key"

# Zilliz Cloud Configuration
CLUSTER_ENDPOINT="your-zilliz-cluster-endpoint"
TOKEN="your-zilliz-token"
COLLECTION_NAME="CodeBase"

# Anthropic API Configuration
ANTHROPIC_API_KEY="your-anthropic-api-key"

# Folder Paths
FOLDER_TO_UPDATE="" ## keep it blank
FOLDER_TO_UPLOAD="src/uploads"

# MCP Server Configuration
MCP_SERVER_HOST="localhost"
MCP_SERVER_PORT="8001"
```

### Configuration Parameters

| Parameter | Description |
|-----------|-------------|
| `OPENAI_API_KEY` | Your OpenAI API key for embeddings and LLM operations |
| `CLUSTER_ENDPOINT` | Zilliz Cloud cluster endpoint URL |
| `TOKEN` | Authentication token for Zilliz Cloud |
| `COLLECTION_NAME` | Name of the collection in Zilliz (default: "CodeBase") |
| `ANTHROPIC_API_KEY` | Your Anthropic API key for Claude integration |
| `FOLDER_TO_UPDATE` | Local folder path where code updates will be applied |
| `FOLDER_TO_UPLOAD` | Folder for storing uploaded ZIP files and extracted code |
| `MCP_SERVER_HOST` | Host for MCP HTTP server (default: "localhost") |
| `MCP_SERVER_PORT` | Port for MCP HTTP server (default: "8001") |

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/RAG_MCP_CODE_SEARCH.git
cd RAG_MCP_CODE_SEARCH
```

### 2ï¸âƒ£ Create and Activate Virtual Environment

**Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the root directory and add your API keys and configuration as shown in the [Environment Configuration](#-environment-configuration) section above.

---

## â–¶ï¸ Running the Project

### Step 1: Start the FastAPI Backend

In a new terminal:

```bash
uvicorn server:app --reload
```

The API server will be available at `http://localhost:8000`

### Step 2: Launch the Streamlit Interface

Open another terminal and run:

```bash
streamlit run streamlit.py
```


---n

### MCP Communication Flow

```
User Query â†’ Streamlit UI â†’ Anthropic Client â†’ MCP HTTP Server
                                                      â†“
                                              [Tool Selection]
                                                      â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â†“                                   â†“
                            RAG Search Tool                     Code Update Tool
                                    â†“                                   â†“
                            Vector DB Query                   Local FS + Vector DB
                                    â†“                                   â†“
                            Return Results              Return Confirmation
                                    â†“                                   â†“
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â†“
                                          MCP HTTP Server Response
                                                      â†“
                                            Anthropic Client
                                                      â†“
                                              User Interface
```

---


### Direct MCP Server API

**RAG Search Endpoint:**
```bash
POST http://localhost:8001/tools/rag_search
Content-Type: application/json

{
  "query": "database connection code"
}
```

**Code Update Endpoint:**
```bash
POST http://localhost:8001/tools/code_update
Content-Type: application/json

{
  "file_path": "src/db.py",
  "content": "updated code content",
  "approved": true
}
```

---

## ğŸ§  Tech Stack

| Component | Technology |
|-----------|-----------|
| **Frontend** | Streamlit |
| **Backend** | FastAPI |
| **Vector Store** | Zilliz Cloud |
| **Model APIs** | OpenAI, Anthropic |
| **Protocol** | Model Context Protocol (MCP) |
| **MCP Server** | Custom Streamable HTTP Server |
| **Language Model** | Claude (via Anthropic API) |
| **RAG Pipeline** | Custom implementation |
| **Deployment** | Uvicorn |

---

## ğŸ“‹ Requirements

- Python 3.8+
- OpenAI API access
- Anthropic API access (Claude)
- Zilliz Cloud account
- Sufficient storage for code indexing
- Network access for MCP server communication

---

## ğŸ” MCP Protocol Benefits

### Why Model Context Protocol?

1. **ğŸ”Œ Standardized Tool Communication**: Consistent interface for AI-tool interactions
2. **ğŸ”„ Stateless Operations**: Each tool call is independent and reproducible
3. **ğŸ“¡ HTTP Streamable**: Real-time streaming responses for large operations
4. **ğŸ§© Modular Design**: Easy to add new tools without changing core architecture
5. **ğŸ¤– AI-Native**: Designed specifically for LLM-tool interactions
6. **ğŸ”’ Secure**: Controlled access to file system and database operations

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Adding New MCP Tools

1. Create a new tool in `mcp/tools/`
2. Implement the tool interface
3. Register the tool in `mcp/server.py`
4. Update the Anthropic client if needed

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ› Troubleshooting

### Common Issues

**Issue**: MCP Server connection failed
- **Solution**: Ensure MCP server is running on the correct port and check firewall settings

**Issue**: Vector DB connection failed
- **Solution**: Verify your `CLUSTER_ENDPOINT` and `TOKEN` in the `.env` file

**Issue**: Indexing takes too long
- **Solution**: This is normal for large codebases. The indexing only needs to be done once.

**Issue**: API rate limits exceeded
- **Solution**: Check your OpenAI/Anthropic API usage limits and upgrade if necessary

**Issue**: MCP tools not responding
- **Solution**: Check MCP server logs and ensure all dependencies are installed correctly

---

## ğŸ“š Resources

- [Model Context Protocol Specification](https://modelcontextprotocol.io/)
- [Anthropic Claude API Documentation](https://docs.anthropic.com/)
- [Zilliz Cloud Documentation](https://docs.zilliz.com/)


---

**Built with â¤ï¸ using RAG, MCP, and Claude AI*
